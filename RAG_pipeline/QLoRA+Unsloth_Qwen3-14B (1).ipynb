{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c99f6f-3b05-4bbe-92b3-a6d59392af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def load_chatml_dataset(data_path: str, tokenizer):\n",
    "\n",
    "    # Load and convert dataset fomrat \n",
    "    ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"qwen3\",\n",
    "    )\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "      convos = examples[\"messages\"]\n",
    "      texts = [tokenizer.apply_chat_template(\n",
    "         convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "      return { \"text\" : texts, }\n",
    "\n",
    "    ds = ds.map(formatting_prompts_func,remove_columns=ds.column_names,batched = True)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834dcc6-3094-4d45-ac44-e222fdca88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.402 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f418601b2754a20971f207fb1dd91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model Config\n",
    "MODEL_NAME = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "DATASET_PATH = \"training_data_cleaned.jsonl\" \n",
    "OUTPUT_DIR = \"./qwen_qlora_finetuned\"\n",
    "MAX_SEQ_LENGTH = 4096  # Gi·∫£m n·∫øu g·∫∑p l·ªói memory\n",
    "\n",
    "# Load m√¥ h√¨nh v·ªõi QLoRA\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True, \n",
    "    token=None,  \n",
    ")\n",
    "\n",
    "print(\"Model loaded OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359e9ed-e0ed-4902-ba12-228cb5f5937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Loading dataset...\")\n",
    "\n",
    "full_ds = load_chatml_dataset(\n",
    "    data_path=DATASET_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "full_ds = full_ds.train_test_split(test_size=0.015, seed=42)\n",
    "train_ds = full_ds[\"train\"]\n",
    "val_ds   = full_ds[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c9d33-6ec9-4d55-8910-8f2957bd5fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8458\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83e9b3-7e01-45c8-bd77-f1af2ab1aa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying QLoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Applying QLoRA adapters...\")\n",
    "\n",
    "# C·∫•u h√¨nh LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  \n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ff838-bbb3-4d18-b090-c1103c650aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Training arguments...\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è Training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    eval_strategy=\"steps\",   \n",
    "    eval_steps=200,                \n",
    "    load_best_model_at_end=True,   \n",
    "    logging_steps=20,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54190089-476a-439f-8bd7-6c2eecb78744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec69288efe9a4cff8c979b664c78c868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=52):   0%|          | 0/8458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378f6ed8f08142f7b6fba44954d65343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=52):   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# T·∫°o trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9006a3-4808-451b-adbc-35b2129dfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running baseline evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 08:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.173104763031006,\n",
       " 'eval_model_preparation_time': 0.007,\n",
       " 'eval_runtime': 14.3166,\n",
       " 'eval_samples_per_second': 9.011,\n",
       " 'eval_steps_per_second': 4.54}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üîç Running baseline evaluation...\")\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae5eda-e797-4a9a-80f3-2ef11aabb4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,458 | Num Epochs = 2 | Total steps = 2,116\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2116' max='2116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2116/2116 1:29:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.438781</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.347120</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.293704</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.248026</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.213128</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.177855</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.168927</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.157985</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758cdc7-91eb-44ca-ae09-beb9074de5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving LoRA adapters...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qwen_qlora_finetuned/tokenizer_config.json',\n",
       " './qwen_qlora_finetuned/special_tokens_map.json',\n",
       " './qwen_qlora_finetuned/chat_template.jinja',\n",
       " './qwen_qlora_finetuned/vocab.json',\n",
       " './qwen_qlora_finetuned/merges.txt',\n",
       " './qwen_qlora_finetuned/added_tokens.json',\n",
       " './qwen_qlora_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üíæ Saving LoRA adapters...\")\n",
    "\n",
    "# L∆∞u model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f22a6a-785c-4720-9b58-9ae2d4fa1d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.402 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b9320e12be4aa4ae88a0b98e6ebbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 5120, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (7-18): 12 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (19): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (20-37): 18 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (38): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "          (39): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=17408, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=17408, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=17408, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=17408, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# 1. Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=4096,\n",
    ")\n",
    "\n",
    "# 2. Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"./qwen_qlora_finetuned\",\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af124f-37cf-44ff-9b01-d19ede958f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n du l·ªãch Vi·ªát Nam chuy√™n nghi·ªáp. \n",
      "user\n",
      "ƒëi h√† n·ªôi n√™n ƒÉn g√¨?\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Th∆∞·ªùng qu√°n r·∫•t ƒë√¥ng n√™n quy t·∫Øc ph·ª•c v·ª• kh√° nghi√™m ng·∫∑t nh·ªÉ. B·∫°n n√™n t·∫≠p trung th∆∞·ªüng th·ª©c b√°t ph·ªü n√≥ng h·ªïi t·∫°i 49 B√°t ƒê√†n ƒë·ªÉ c·∫£m nh·∫≠n tr·ªçn v·∫πn v·ªã ngon.\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n du l·ªãch Vi·ªát Nam chuy√™n nghi·ªáp. \"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"ƒëi h√† n·ªôi n√™n ƒÉn g√¨?\"\n",
    "    }\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43e4b9-5d19-43ee-ae67-ba32d6d17a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAHWCAYAAABkNgFvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZiZJREFUeJzt3Xd4VFXCx/HvnZJJnTRSSei9CooIKKJ0EUWsyL6C+9rB1bXt+roq4Korrrqu3XVX1BXUdQU7EsFQBFGkqIgU6ZBCC+nJZOa+f4SMxBRCMplJwu/zPPdJ5t5z75w7OZKfJ+eeY5imaSIiIiIi0gxZAl0BEREREZH6UpgVERERkWZLYVZEREREmi2FWRERERFpthRmRURERKTZUpgVERERkWZLYVZEREREmi2FWRERERFpthRmRURERKTZUpgVkSZh6tSptGvXrl7nzpgxA8MwfFuhZi49PR3DMEhPT/fuq+tnvHPnTgzDYM6cOT6tU7t27Zg6dapPrykiojArIrUyDKNO2/Gh6VTj8Xj461//SufOnQkJCaFjx47cfPPN5Ofn1+n8Pn360KZNG2pbXXzIkCEkJCRQVlbmq2o3ipUrVzJjxgxycnICXRWvOXPmYBgGa9asCXRVRKQR2AJdARFp2t54441Kr19//XXS0tKq7O/evXuD3ucf//gHHo+nXuf+6U9/4o9//GOD3r8hnn76ae6++24mTJjA3Xffza5du5g3bx5/+MMfCA8PP+H5kydP5o9//CPLly9n6NChVY7v3LmTVatWMX36dGy2+v+z3ZDPuK5WrlzJzJkzmTp1KlFRUZWObd68GYtFfSgi4lsKsyJSq9/85jeVXn/11VekpaVV2f9rhYWFhIaG1vl97HZ7veoHYLPZGhTyGuqtt96iZ8+evPfee97hDg899FCdg+PVV1/Nvffey9y5c6sNs/PmzcM0TSZPntygejbkM/YFh8MR0PcXkZZJ/4ssIg02bNgwevXqxbfffsvQoUMJDQ3l//7v/wB4//33GTduHMnJyTgcDjp27MhDDz2E2+2udI1fj+esGLf517/+lZdffpmOHTvicDgYMGAA33zzTaVzqxszaxgG06dPZ8GCBfTq1QuHw0HPnj1ZuHBhlfqnp6dzxhlnEBwcTMeOHXnppZdOahyuxWLB4/FUKm+xWOocsFNTUxk6dCjvvvsuLperyvG5c+fSsWNHBg4cyK5du7jlllvo2rUrISEhxMbGcvnll7Nz584Tvk91Y2ZzcnKYOnUqkZGRREVFMWXKlGqHCHz33XdMnTqVDh06EBwcTGJiIr/97W85dOiQt8yMGTO4++67AWjfvr13CEpF3aobM7t9+3Yuv/xyYmJiCA0N5ayzzuLjjz+uVKZi/O8777zDww8/TEpKCsHBwQwfPpxt27ad8L7rat26dYwdOxan00l4eDjDhw/nq6++qlTG5XIxc+ZMOnfuTHBwMLGxsZx99tmkpaV5y2RmZnLttdeSkpKCw+EgKSmJiy++uE4/IxE5eeqZFRGfOHToEGPHjuWqq67iN7/5DQkJCUD5eMXw8HDuuOMOwsPDWbJkCQ888AC5ubk8/vjjJ7zu3LlzycvL48Ybb8QwDGbPns3EiRPZvn37CXsaV6xYwXvvvcctt9xCREQEf//737n00kvZvXs3sbGxQHmAGTNmDElJScycORO3282sWbOIi4ur871fe+213Hjjjbz00kvceOONdT7veJMnT+aGG27gs88+48ILL/Tu//777/nhhx944IEHAPjmm29YuXIlV111FSkpKezcuZMXXniBYcOG8eOPP55Ub7hpmlx88cWsWLGCm266ie7duzN//nymTJlSpWxaWhrbt2/n2muvJTExkY0bN/Lyyy+zceNGvvrqKwzDYOLEiWzZsoV58+bx1FNP0apVK4AaP8usrCwGDx5MYWEhv/vd74iNjeW1117joosu4t133+WSSy6pVP4vf/kLFouFu+66i6NHjzJ79mwmT57M6tWr63zPNdm4cSPnnHMOTqeTe+65B7vdzksvvcSwYcNYunQpAwcOBMoD+6OPPsp1113HmWeeSW5uLmvWrGHt2rWMHDkSgEsvvZSNGzdy66230q5dO7Kzs0lLS2P37t31fshRRGphioichGnTppm//qfj3HPPNQHzxRdfrFK+sLCwyr4bb7zRDA0NNYuLi737pkyZYrZt29b7eseOHSZgxsbGmocPH/buf//9903A/PDDD737HnzwwSp1AsygoCBz27Zt3n0bNmwwAfOZZ57x7hs/frwZGhpq7tu3z7tv69atps1mq3LNmvzxj380g4KCTKvVar733nt1OufXDh8+bDocDnPSpElVrg2YmzdvNk2z+s9z1apVJmC+/vrr3n1ffPGFCZhffPGFd9+vP+MFCxaYgDl79mzvvrKyMvOcc84xAfPVV1/17q/ufefNm2cC5rJly7z7Hn/8cRMwd+zYUaV827ZtzSlTpnhf33777SZgLl++3LsvLy/PbN++vdmuXTvT7XZXupfu3bubJSUl3rJPP/20CZjff/99lfc63quvvmoC5jfffFNjmQkTJphBQUHmzz//7N23f/9+MyIiwhw6dKh3X9++fc1x48bVeJ0jR46YgPn444/XWicR8R0NMxARn3A4HFx77bVV9oeEhHi/z8vL4+DBg5xzzjkUFhby008/nfC6V155JdHR0d7X55xzDlD+5+kTGTFiBB07dvS+7tOnD06n03uu2+3m888/Z8KECSQnJ3vLderUibFjx57w+gB///vfefLJJ/nyyy+ZNGkSV111FYsWLapUxuFwcP/999d6nejoaC644AI++OADCgoKgPKe07feeoszzjiDLl26AJU/T5fLxaFDh+jUqRNRUVGsXbu2TnWu8Mknn2Cz2bj55pu9+6xWK7feemuVsse/b3FxMQcPHuSss84COOn3Pf79zzzzTM4++2zvvvDwcG644QZ27tzJjz/+WKn8tddeS1BQkPf1ybSF2rjdbhYtWsSECRPo0KGDd39SUhJXX301K1asIDc3F4CoqCg2btzI1q1bq71WSEgIQUFBpKenc+TIkQbVS0TqRmFWRHyidevWlYJGhY0bN3LJJZcQGRmJ0+kkLi7O+/DY0aNHT3jdNm3aVHpdEWzrEhR+fW7F+RXnZmdnU1RURKdOnaqUq27frxUVFfHggw9y3XXXccYZZ/Dqq69y/vnnc8kll7BixQoAtm7dSmlpqffP1LWZPHkyBQUFvP/++0D5zAA7d+6s9OBXUVERDzzwAKmpqTgcDlq1akVcXBw5OTl1+jyPt2vXLpKSkqrMuNC1a9cqZQ8fPsxtt91GQkICISEhxMXF0b59e6BuP8ea3r+696qYGWPXrl2V9jekLdTmwIEDFBYW1lgXj8fDnj17AJg1axY5OTl06dKF3r17c/fdd/Pdd995yzscDh577DE+/fRTEhISGDp0KLNnzyYzM7NBdRSRminMiohPHN9zVyEnJ4dzzz2XDRs2MGvWLD788EPS0tJ47LHHAOr0tL/Vaq12v1nLnKy+OLcuNm3aRE5OjreH0maz8e6779KrVy/GjRvH2rVrefnll4mPj/eOp6zNhRdeSGRkJHPnzgXKxwtbrVauuuoqb5lbb72Vhx9+mCuuuIJ33nmHRYsWkZaWRmxsbKNOu3XFFVfwj3/8g5tuuon33nuPRYsWeR+ma+zpvio09s+zLoYOHcrPP//Mv/71L3r16sUrr7xC//79eeWVV7xlbr/9drZs2cKjjz5KcHAw999/P927d2fdunV+q6fIqUQPgIlIo0lPT+fQoUO89957laac2rFjRwBr9Yv4+HiCg4OrfSK+Lk/JV8xeUNFrBxAWFsYnn3zC2WefzejRoykuLubPf/5znaalcjgcXHbZZbz++utkZWXxn//8h/PPP5/ExERvmXfffZcpU6bwxBNPePcVFxfXa5GCtm3bsnjxYvLz8yv1zm7evLlSuSNHjrB48WJmzpzpfRANqPZP7SezElvbtm2rvBfgHX7Stm3bOl+rIeLi4ggNDa2xLhaLhdTUVO++mJgYrr32Wq699lry8/MZOnQoM2bM4LrrrvOW6dixI3feeSd33nknW7du5bTTTuOJJ57g3//+t1/uSeRUop5ZEWk0FT1px/eclZaW8vzzzweqSpVYrVZGjBjBggUL2L9/v3f/tm3b+PTTT094fu/evUlISODZZ58lOzvbuz82NpZXX32VgwcPUlRUxPjx4+tcp8mTJ+Nyubjxxhs5cOBAlbllrVZrlZ7IZ555pspUZ3VxwQUXUFZWxgsvvODd53a7eeaZZ6q8J1TtAf3b3/5W5ZphYWEAdQrXF1xwAV9//TWrVq3y7isoKODll1+mXbt29OjRo6630iBWq5VRo0bx/vvvV5o+Kysri7lz53L22WfjdDoBKk1FBuVjfDt16kRJSQlQPr9ycXFxpTIdO3YkIiLCW0ZEfEs9syLSaAYPHkx0dDRTpkzhd7/7HYZh8MYbb/j1z8InMmPGDBYtWsSQIUO4+eabcbvdPPvss/Tq1Yv169fXeq7NZuPZZ5/lyiuvpHfv3tx44420bduWTZs28a9//YvevXuzd+9eLr74Yr788ktvIKrNueeeS0pKCu+//z4hISFMnDix0vELL7yQN954g8jISHr06MGqVav4/PPPvVONnYzx48czZMgQ/vjHP7Jz50569OjBe++9V2UMrNPp9I79dLlctG7dmkWLFlXbw3766acDcN9993HVVVdht9sZP368N+Qe749//CPz5s1j7Nix/O53vyMmJobXXnuNHTt28N///tfnq4X961//qnae4dtuu40///nPpKWlcfbZZ3PLLbdgs9l46aWXKCkpYfbs2d6yPXr0YNiwYZx++unExMSwZs0a3n33XaZPnw7Ali1bGD58OFdccQU9evTAZrMxf/58srKyKg0XERHfUZgVkUYTGxvLRx99xJ133smf/vQnoqOj+c1vfsPw4cMZPXp0oKsHlIevTz/9lLvuuov777+f1NRUZs2axaZNm+o028Jll11Geno6Dz/8ME8//TQlJSV07tyZe+65h9tuu42lS5cybtw4Lr/8cj7++OMTLqRgsViYNGkSjz/+OOPHjyciIqLS8aeffhqr1cqbb75JcXExQ4YM4fPPP6/X52mxWPjggw+4/fbb+fe//41hGFx00UU88cQT9OvXr1LZuXPncuutt/Lcc89hmiajRo3i008/rTQLBMCAAQN46KGHePHFF1m4cCEej4cdO3ZUG2YTEhJYuXIlf/jDH3jmmWcoLi6mT58+fPjhh4wbN+6k7+dEju+BPt7UqVPp2bMny5cv59577+XRRx/F4/EwcOBA/v3vf1d6eO93v/sdH3zwAYsWLaKkpIS2bdvy5z//2btYRGpqKpMmTWLx4sW88cYb2Gw2unXrxjvvvMOll17q83sSETDMptRFIiLSREyYMKHWKZhERKRp0JhZETnlFRUVVXq9detWPvnkE4YNGxaYComISJ2pZ1ZETnlJSUlMnTqVDh06sGvXLl544QVKSkpYt24dnTt3DnT1RESkFhozKyKnvDFjxjBv3jwyMzNxOBwMGjSIRx55REFWRKQZUM+siIiIiDRbGjMrIiIiIs2WwqyIiIiINFun3JhZj8fD/v37iYiIOKllF0VERETEP0zTJC8vj+Tk5BMuoHLKhdn9+/dXWmNbRERERJqmPXv2kJKSUmuZUy7MVqyms2fPnjotLXkyXC4XixYtYtSoUdjtdp9eW5o/tQ+pidqG1EbtQ2rSkttGbm4uqampVVZBrM4pF2YrhhY4nc5GCbOhoaE4nc4W16ik4dQ+pCZqG1IbtQ+pyanQNuoyJFQPgImIiIhIs6UwKyIiIiLNlsKsiIiIiDRbp9yYWREREWneTNOkrKwMt9sd6KoElMvlwmazUVxc3Cw/C7vdjtVqbfB1FGZFRESk2SgtLSUjI4PCwsJAVyXgTNMkMTGRPXv2NMu58w3DICUlhfDw8AZdR2FWREREmgWPx8OOHTuwWq0kJycTFBTULEOcr3g8HvLz8wkPDz/hwgJNjWmaHDhwgL1799K5c+cG9dAqzIqIiEizUFpaisfjITU1ldDQ0EBXJ+A8Hg+lpaUEBwc3uzALEBcXx86dO3G5XA0Ks83vzkVEROSU1hyDm1Tlq151tQYRERERabYUZkVERESk2VKYFREREWlG2rVrx9/+9jefXCs9PR3DMMjJyfHJ9QJBD4CJiIiINLJhw4Zx2mmn+SSEfvPNN4SFhTW8Ui2EwqyIiIhIgJmmidvtxmY7cTSLi4sDymczEA0zaHTPLN7K6KeW8e+vdgW6KiIiIi2OaZoUlpYFZDNNs051nDp1KkuXLuXpp5/GMAwMw2DOnDkYhsGnn37K6aefjsPhYMWKFfz8889cfPHFJCQkEB4ezoABA/j8888rXe/XwwysViuvvPIKl1xyCaGhoXTu3JkPPvig3p/pf//7X3r27InD4aBdu3Y88cQTlY4///zzdO7cmeDgYBISErjsssu8x95991169+5NSEgIsbGxjBgxgoKCgnrXpS7UM9vIDuSXsDkrj+zc4kBXRUREpMUpcrnp8cBnAXnvH2eNJjToxFHq6aefZsuWLfTq1YtZs2YBsHHjRgD++Mc/8te//pUOHToQHR3Nnj17uOCCC3j44YdxOBy8/vrrjB8/ns2bN9OmTZsa32PmzJnMnj2bxx9/nGeeeYbJkyeza9cuYmJiTuqevv32W6644gpmzJjBlVdeycqVK7nllluIjY1l6tSprFmzht/97ne88cYbDB48mMOHD7N8+XIAMjIymDRpErNnz+aSSy4hLy+P5cuX1zn015fCbCOzW8s7v0vdjfuDFBERkaYpMjKSoKAgQkNDSUxMBOCnn34CYNasWYwcOdJbNiYmhr59+3pfP/TQQ8yfP58PPviA6dOn1/geU6dOZdKkSQA88sgj/P3vf+frr79mzJgxJ1XXJ598kuHDh3P//fcD0KVLF3788Ucef/xxpk6dyu7duwkLC+PCCy8kIiKCtm3b0q9fP6A8zJaVlTFx4kTatm0LQO/evU/q/etDYbaRBdmOhdkyjWsRERHxtRC7lR9njQ7YezfUGWecUel1fn4+M2bM4OOPP/aGw6KiInbv3l3rdfr06eP9PiwsDKfTSXZ29knXZ9OmTVx88cWV9g0ZMoS//e1vuN1uRo4cSdu2benQoQNjxoxhzJgx3uENffv2Zfjw4fTu3ZvRo0czatQoLrvsMqKjo0+6HidDY2YbWUXPrMutMCsiIuJrhmEQGmQLyOaLFax+PSvBXXfdxfz583nkkUdYvnw569evp3fv3pSWltZ6HbvdXuVzaYwHxCIiIli7di3z5s0jKSmJBx54gL59+5KTk4PVaiUtLY1PP/2UHj168Mwzz9C1a1d27Njh83ocT2G2kTnUMysiInLKCwoKwu12n7Dcl19+ydSpU7nkkkvo3bs3iYmJ7Ny5s/EreEz37t358ssvq9SpS5cuWK3lPdE2m40RI0Ywe/ZsvvvuO3bu3MmSJUuA8hA9ZMgQZs6cybp16wgKCmL+/PmNWmcNM2hkQeqZFREROeW1a9eO1atXs3PnTsLDw2vsNe3cuTPvvfce48ePxzAM7r//fr9OwXXnnXcyYMAAHnroIa688kpWrVrFs88+y/PPPw/ARx99xPbt2xk6dCjR0dF88skneDweunbtyurVq1m8eDGjRo0iPj6e1atXc+DAAbp3796odVbPbCOzW8v/BFGiMCsiInLKuuuuu7BarfTo0YO4uLgax8A++eSTREdHM3jwYMaPH8/o0aPp37+/3+rZv39/3nnnHd566y169erFAw88wKxZs5g6dSoAUVFRvPfee5x//vl0796dF198kXnz5tGzZ0+cTifLli3jggsuoEuXLvzpT3/iiSeeYOzYsY1aZ/XMNrIgW3mXvIYZiIiInLq6dOnCqlWrKu2rCIjHa9eunfdP9hWmTZtW6XXFsIOKHlu3243FUrl/sq7L0w4bNqzK1FmXXnopl156abXlzz77bNLT06s91r17dxYuXFin9/Ul9cw2soqeWQ0zEBEREfE9hdlGpqm5REREJFBuuukmwsPDq91uuummQFfPJzTMoJHpATAREREJlFmzZnHXXXdVe8zpdPq5No1DYbaRqWdWREREAiU+Pp74+PhAV6NRaZhBI9NytiIiIiKNR2G2kf3SM3viiZJFRERE5OQozDayijDrUs+siIiIiM8pzDayigfANGZWRERExPcUZhuZd5iBZjMQERER8TmF2UZW8QCYSz2zIiIiUk/t2rXjb3/7W53KGobBggULGrU+TYnCbCOr6JktUc+siIiIiM8pzDay45ez/fXaxyIiIiLSMAqzjcxhtQJgmlDmUZgVERHxKdOE0oLAbHXspHr55ZdJTk7G46n8V9qLL76Y3/72t/z8889cfPHFJCQkEB4ezoABA/j888999hF9//33nH/++YSEhBAbG8sNN9xAfn6+93h6ejpnnnkmYWFhREVFMWTIEHbt2gXAhg0bOO+884iIiMDpdHL66aezZs0an9XNF7QCWCOrGGYA5b2zFWNoRURExAdchfBIcmDe+//2Q1DYCYtdfvnl3HrrrXzxxRcMHz4cgMOHD7Nw4UI++eQT8vPzueCCC3j44YdxOBy8/vrrjB8/ns2bN9OmTZsGVbGgoIDRo0czaNAgvvnmG7Kzs7nuuuuYPn06c+bMoaysjAkTJnD99dczb948SktL+frrrzGM8r8sT548mX79+vHCCy9gtVpZv349dru9QXXyNYXZRlYxzADKp+cKDQpgZURERMTvoqOjGTt2LHPnzvWG2XfffZdWrVpx3nnnYbFY6Nu3r7f8Qw89xPz58/nggw+YPn16g9577ty5FBcX8/rrrxMWVh68n332WcaPH89jjz2G3W7n6NGjXHjhhXTs2BGA7t27e8/fvXs3d999N926dQOgc+fODapPY1CYbWQ2qwWLAR5Tc82KiIj4nD20vIc0UO9dR5MnT+b666/n+eefx+Fw8Oabb3LVVVdhsVjIz89nxowZfPzxx2RkZFBWVkZRURG7d+9ucBU3bdpE3759vUEWYMiQIXg8HjZv3szQoUOZOnUqo0ePZuTIkYwYMYIrrriCpKQkAO644w6uu+463njjDUaMGMHll1/uDb1Nhf7m7QcVQws016yIiIiPGUb5n/oDsRnGiet3zPjx4zFNk48//pg9e/awfPlyJk+eDMBdd93F/PnzeeSRR1i+fDnr16+nd+/elJaWNtanVsmrr77KqlWrGDx4MG+//TZdunThq6++AmDGjBls3LiRcePGsWTJEnr06MH8+fP9Uq+6Upj1A+/CCeqZFREROSUFBwczceJE3nzzTebNm0fXrl3p378/AF9++SVTp07lkksuoXfv3iQmJrJz506fvG/37t3ZsGEDBQUF3n1ffvklFouFrl27evf169ePe++9l5UrV9KrVy/mzp3rPdalSxd+//vfs2jRIiZOnMirr77qk7r5isKsH1QsaetyazYDERGRU9XkyZP5+OOP+de//uXtlYXycajvvfce69evZ8OGDVx99dVVZj5oyHsGBwczZcoUfvjhB7744gtuvfVW/ud//oeEhAR27NjBvffey6pVq9i1axeLFi1i69atdO/enaKiIqZPn056ejq7du3iyy+/5Jtvvqk0prYp0JhZP1DPrIiIiJx//vnExMSwefNmrr76au/+J598kt/+9rcMHjyYVq1a8Yc//IHc3FyfvGdoaCifffYZt912GwMGDCA0NJRLL72UJ5980nv8p59+4rXXXuPQoUMkJSUxbdo0brzxRsrKyjh06BDXXHMNWVlZtGrViokTJzJz5kyf1M1XFGb9QGNmRURExGKxsH9/1YfV2rVrx5IlSyrtmzZtWqXXJzPs4NeLNPXu3bvK9SskJCTUOAY2KCiIefPm1fl9A0XDDPxAPbMiIiIijUNh1g9+GTOrMCsiIiL19+abbxIeHk54eDhOp5OUlBScTifh4eH07Nkz0NULCA0z8AO7emZFRETEBy666CIGDhwIgMfjIT8/n/DwcCwWS5NbmctfFGb9wKExsyIiIuIDERERREREAOVhNjc3F6fTicVy6v6x/dS9cz+y28onVdYwAxERkYb79QNO0jz56ueoMOsHFWNmSzTMQEREpN4q/oxeWFgY4JqIL1SscGa1Wht0HQ0z8AO7HgATERFpMKvVSlRUFNnZ2UD5HKnGSSwp29J4PB5KS0spLi5udsMMPB4PBw4cIDQ0FJutYXFUYdYPNDWXiIiIbyQmJgJ4A+2pzDRNioqKCAkJaZah3mKx0KZNmwbXXWHWDyrCrHpmRUREGsYwDJKSkoiPj8flcgW6OgHlcrlYtmwZQ4cObZYzGQQFBfmkR1lh1g8qxsyqZ1ZERMQ3rFZrg8daNndWq5WysjKCg4ObZZj1lYAOsHj00UcZMGAAERERxMfHM2HCBDZv3lzrOXPmzMEwjEpbcHCwn2pcPxpmICIiItI4Ahpmly5dyrRp0/jqq69IS0vD5XIxatQoCgoKaj3P6XSSkZHh3Xbt2uWnGteP3TvPrKYSEREREfGlgA4zWLhwYaXXc+bMIT4+nm+//ZahQ4fWeJ5hGN4B4M2BemZFREREGkeTGjN79OhRAGJiYmotl5+fT9u2bfF4PPTv359HHnmkxvWIS0pKKCkp8b7Ozc0FygdN+3rgeMX1fn1dK+U9siWuslN+sPqprKb2IaK2IbVR+5CatOS2cTL3ZJhNZBkNj8fDRRddRE5ODitWrKix3KpVq9i6dSt9+vTh6NGj/PWvf2XZsmVs3LiRlJSUKuVnzJjBzJkzq+yfO3cuoaGhPr2Hmizaa/DxHitnxXuY1FG9syIiIiK1KSws5Oqrr+bo0aM4nc5ayzaZMHvzzTfz6aefsmLFimpDaU1cLhfdu3dn0qRJPPTQQ1WOV9czm5qaysGDB0/44Zwsl8tFWloaI0eOrPRU4SsrdvLYZ1uY0DeJxy/r7dP3lOajpvYhorYhtVH7kJq05LaRm5tLq1at6hRmm8Qwg+nTp/PRRx+xbNmykwqyUL60Xb9+/di2bVu1xx0OBw6Ho9rzGusH/+trhwSVf8wukxbX2OTkNWbbk+ZNbUNqo/YhNWmJbeNk7iegsxmYpsn06dOZP38+S5YsoX379id9Dbfbzffff09SUlIj1NA3gmzl8+C59ACYiIiIiE8FtGd22rRpzJ07l/fff5+IiAgyMzMBiIyMJCQkBIBrrrmG1q1b8+ijjwIwa9YszjrrLDp16kROTg6PP/44u3bt4rrrrgvYfZyI3Vq+TFupVgATERER8amAhtkXXngBgGHDhlXa/+qrrzJ16lQAdu/eXWmpsyNHjnD99deTmZlJdHQ0p59+OitXrqRHjx7+qvZJ09RcIiIiIo0joGG2Ls+epaenV3r91FNP8dRTTzVSjRpHxXK2LvXMioiIiPhUQMfMnirUMysiIiLSOBRm/UDL2YqIiIg0DoVZP/ilZ9Yd4JqIiIiItCwKs35QEWZd6pkVERER8SmFWT+oeABMY2ZFREREfEth1g9+6ZlVmBURERHxJYVZP7CrZ1ZERESkUSjM+kFFz2yJemZFREREfEph1g8qlrN1uT11WihCREREROpGYdYPHFYrAKYJZR6FWRERERFfUZj1g4phBqCHwERERER8SWHWDyqGGYAeAhMRERHxJYVZP7BZLViO5dlS9cyKiIiI+IzCrJ9oei4RERER31OY9ZOKcbMKsyIiIiK+ozDrJxVL2rrcms1ARERExFcUZv1EPbMiIiIivqcw6yfeMbN6AExERETEZxRm/UQ9syIiIiK+pzDrJ7+MmVWYFREREfEVhVk/satnVkRERMTnFGb9xKGeWRERERGfU5j1E7utfAkwPQAmIiIi4jsKs35SMWa2RMMMRERERHxGYdZP7BpmICIiIuJzCrN+oqm5RERERHxPYdZPKsKsemZFREREfEdh1k8qxsyqZ1ZERETEdxRm/cQ7zMBtBrgmIiIiIi2Hwqyf2NUzKyIiIuJzCrN+ojGzIiIiIr6nMOsn6pkVERER8T2FWT9xaGouEREREZ9TmPUTu7V8OVsNMxARERHxHYVZP/EuZ6swKyIiIuIzCrN+EmSzAuDSMAMRERERn1GY9ZOKYQal6pkVERER8RmFWT/R1FwiIiIivqcw6ydazlZERETE9xRm/SRIU3OJiIiI+JzCrJ94F01wmwGuiYiIiEjLoTDrJ7/0zLoDXBMRERGRlkNh1k9+eQBMPbMiIiIivqIw6yd6AExERETE9xRm/URTc4mIiIj4nsKsn9jVMysiIiLicwqzfuJ9AEw9syIiIiI+ozDrJ8cvZ2uaeghMRERExBcUZv3EYbUCYJpQ5lGYFREREfEFhVk/sdsM7/d6CExERETENxRm/aRiai7QQ2AiIiIivqIw6yc2qwXLsc5ZPQQmIiIi4hsKs36k6blEREREfEth1o+0pK2IiIiIbwU0zD766KMMGDCAiIgI4uPjmTBhAps3bz7hef/5z3/o1q0bwcHB9O7dm08++cQPtW04LWkrIiIi4lsBDbNLly5l2rRpfPXVV6SlpeFyuRg1ahQFBQU1nrNy5UomTZrE//7v/7Ju3TomTJjAhAkT+OGHH/xY8/rxLpygMCsiIiLiE7ZAvvnChQsrvZ4zZw7x8fF8++23DB06tNpznn76acaMGcPdd98NwEMPPURaWhrPPvssL774YqPXuSG8Y2b1AJiIiIiITwQ0zP7a0aNHAYiJiamxzKpVq7jjjjsq7Rs9ejQLFiyotnxJSQklJSXe17m5uQC4XC5cLlcDa1xZxfVqum7FKmBFJaU+f29p+k7UPuTUpbYhtVH7kJq05LZxMvfUZMKsx+Ph9ttvZ8iQIfTq1avGcpmZmSQkJFTal5CQQGZmZrXlH330UWbOnFll/6JFiwgNDW1YpWuQlpZW7f7iAitg8OWq1RzapIfATlU1tQ8RtQ2pjdqH1KQlto3CwsI6l20yYXbatGn88MMPrFixwqfXvffeeyv15Obm5pKamsqoUaNwOp0+fS+Xy0VaWhojR47EbrdXOf7PPV+xrzCXvv1PZ3i3eJ++tzR9J2ofcupS25DaqH1ITVpy26j4S3pdNIkwO336dD766COWLVtGSkpKrWUTExPJysqqtC8rK4vExMRqyzscDhwOR5X9dru90X7wNV072Fb+cZtYWlyjk7przLYnzZvahtRG7UNq0hLbxsncT0BnMzBNk+nTpzN//nyWLFlC+/btT3jOoEGDWLx4caV9aWlpDBo0qLGq6TN2W/mYWT0AJiIiIuIbAe2ZnTZtGnPnzuX9998nIiLCO+41MjKSkJAQAK655hpat27No48+CsBtt93GueeeyxNPPMG4ceN46623WLNmDS+//HLA7qOuNM+siIiIiG8FtGf2hRde4OjRowwbNoykpCTv9vbbb3vL7N69m4yMDO/rwYMHM3fuXF5++WX69u3Lu+++y4IFC2p9aKyp0NRcIiIiIr4V0J5Z0zzxE/3p6elV9l1++eVcfvnljVCjxqVFE0RERER8K6A9s6eaimEGLvXMioiIiPiEwqwfqWdWRERExLcUZv3IG2bdWjBBRERExBcUZv3IrtkMRERERHxKYdaPKnpmNWZWRERExDcUZv1IPbMiIiIivqUw60cO9cyKiIiI+JTCrB/ZrceWs1XPrIiIiIhPKMz6UcU8syXqmRURERHxCYVZPwqyWQFwqWdWRERExCcUZv3IO8xAPbMiIiIiPqEw60eamktERETEtxRm/ShIU3OJiIiI+JTCrB9pOVsRERER31KY9SMtmiAiIiLiWwqzfuTtmS1zB7gmIiIiIi2Dwqwf/fIAmIYZiIiIiPiCwqwf6QEwEREREd9SmPUjTc0lIiIi4lsKs36kB8BEREREfEth1o9+mZpLYVZERETEFxRm/ej45WxNUw+BiYiIiDSUwqwfOaxWAEwT3B6FWREREZGGUpj1I7vN8H6voQYiIiIiDacw60cVU3OBHgITERER8QWFWT+yWS1YjnXOqmdWREREpOEUZv1M03OJiIiI+I7CrJ9pSVsRERER31GY9TMtaSsiIiLiOwqzfqYlbUVERER8R2HWzyrGzJaoZ1ZERESkwRRm/cy7pK3CrIiIiEiDKcz6WcWYWQ0zEBEREWk4hVk/s6tnVkRERMRnFGb9zKGeWRERERGfUZj1M7utfAkwrQAmIiIi0nAKs36meWZFREREfEdh1s+8y9mqZ1ZERESkwRRm/cy7aIJ6ZkVEREQaTGHWz4LUMysiIiLiMwqzfqZFE0RERER8R2HWz7xh1m0GuCYiIiIizZ/CrJ/ZNZuBiIiIiM8ozPqZ9wEwjZkVERERaTCFWT9Tz6yIiIiI7yjM+plDPbMiIiIiPqMw62d267HlbNUzKyIiItJgCrN+pnlmRURERHxHYdbPgmxWQD2zIiIiIr5QrzC7Z88e9u7d63399ddfc/vtt/Pyyy/7rGItlXeYgXpmRURERBqsXmH26quv5osvvgAgMzOTkSNH8vXXX3Pfffcxa9Ysn1awpdHUXCIiIiK+U68w+8MPP3DmmWcC8M4779CrVy9WrlzJm2++yZw5c3xZvxYnSFNziYiIiPhMvcKsy+XC4XAA8Pnnn3PRRRcB0K1bNzIyMnxXuxZIy9mKiIiI+E69wmzPnj158cUXWb58OWlpaYwZMwaA/fv3Exsb69MKtjRaNEFERETEd+oVZh977DFeeuklhg0bxqRJk+jbty8AH3zwgXf4QV0sW7aM8ePHk5ycjGEYLFiwoNby6enpGIZRZcvMzKzPbQSExsyKiIiI+I6tPicNGzaMgwcPkpubS3R0tHf/DTfcQGhoaJ2vU1BQQN++ffntb3/LxIkT63ze5s2bcTqd3tfx8fF1PjfQ1DMrIiIi4jv1CrNFRUWYpukNsrt27WL+/Pl0796d0aNH1/k6Y8eOZezYsSf9/vHx8URFRZ30eU1BxXK2CrMiIiIiDVevMHvxxRczceJEbrrpJnJychg4cCB2u52DBw/y5JNPcvPNN/u6npWcdtpplJSU0KtXL2bMmMGQIUNqLFtSUkJJSYn3dW5uLlD+EJvL5fJpvSquV9t1LWZ5iC11u33+/tK01aV9yKlJbUNqo/YhNWnJbeNk7qleYXbt2rU89dRTALz77rskJCSwbt06/vvf//LAAw80WphNSkrixRdf5IwzzqCkpIRXXnmFYcOGsXr1avr371/tOY8++igzZ86ssn/RokUnNSTiZKSlpdV4LLsIwEZhcSmffPJJo7y/NG21tQ85taltSG3UPqQmLbFtFBYW1rmsYZrmSc8RFRoayk8//USbNm244oor6NmzJw8++CB79uyha9euJ1UBb0UMg/nz5zNhwoSTOu/cc8+lTZs2vPHGG9Uer65nNjU1lYMHD1Yad+sLLpeLtLQ0Ro4cid1ur7bM/pwizn1iOQ6bhR8eHOHT95emrS7tQ05NahtSG7UPqUlLbhu5ubm0atWKo0ePnjCv1atntlOnTixYsIBLLrmEzz77jN///vcAZGdn+zwgnsiZZ57JihUrajzucDi8c+Iez263N9oPvrZrhwS7gfLlbG02G4ZhNEodpOlqzLYnzZvahtRG7UNq0hLbxsncT72m5nrggQe46667aNeuHWeeeSaDBg0Cyv90369fv/pcst7Wr19PUlKSX9+zIRxWKwCmCW6PFk4QERERaYh69cxedtllnH322WRkZHjnmAUYPnw4l1xySZ2vk5+fz7Zt27yvd+zYwfr164mJiaFNmzbce++97Nu3j9dffx2Av/3tb7Rv356ePXtSXFzMK6+8wpIlS1i0aFF9biMg7LZfemJL3R5s1nr9/4SIiIiIUM8wC5CYmEhiYiJ79+4FICUl5aQWTABYs2YN5513nvf1HXfcAcCUKVOYM2cOGRkZ7N6923u8tLSUO++8k3379hEaGkqfPn34/PPPK12jqQs6Lry6ykwICmBlRERERJq5eoVZj8fDn//8Z5544gny8/MBiIiI4M477+S+++7DYqlbb+OwYcOo7fmzOXPmVHp9zz33cM8999Snyk2GzWrBYoDHhBK3G2hZY1xERERE/KleYfa+++7jn//8J3/5y1+8c7yuWLGCGTNmUFxczMMPP+zTSrY0dquFkjKPFk4QERERaaB6hdnXXnuNV155hYsuusi7r0+fPrRu3ZpbbrlFYfYEgmzlYdbl1gNgIiIiIg1Rr6ePDh8+TLdu3ars79atG4cPH25wpVq6inGz6pkVERERaZh6hdm+ffvy7LPPVtn/7LPP0qdPnwZXqqULspV/7C63wqyIiIhIQ9RrmMHs2bMZN24cn3/+uXeO2VWrVrFnzx4t0VoH9mM9syXqmRURERFpkHr1zJ577rls2bKFSy65hJycHHJycpg4cSIbN26scVlZ+YV6ZkVERER8o97zzCYnJ1d50GvDhg3885//5OWXX25wxVoyu8bMioiIiPiElp8KgIqeWYVZERERkYZRmA0Ah1XDDERERER8QWE2AOw2A4BShVkRERGRBjmpMbMTJ06s9XhOTk5D6nLK0DyzIiIiIr5xUmE2MjLyhMevueaaBlXoVOB9AEw9syIiIiINclJh9tVXX22sepxSvFNzqWdWREREpEE0ZjYAgtQzKyIiIuITCrMB8MuiCWaAayIiIiLSvCnMBkBFmNVytiIiIiINozAbAFoBTERERMQ3FGYD4JdhBgqzIiIiIg2hMBsA6pkVERER8Q2F2QBwqGdWRERExCcUZgPAbj22nK16ZkVEREQaRGE2ADTPrIiIiIhvKMwGQJDNCqhnVkRERKShFGYDoGKYgcbMioiIiDSMwmwAVEzNpWEGIiIiIg2jMBsAQZqaS0RERMQnFGYD4JeeWTPANRERERFp3hRmA0CLJoiIiIj4hsJsAGg5WxERERHfUJgNAPXMioiIiPiGwmwAaDlbEREREd9QmA2AimEGxS53gGsiIiIi0rwpzAZAdGgQADlFLsrUOysiIiJSbwqzARAbFoTVYmCacKigNNDVEREREWm2FGYDwGIxaBVe3jubnVsS4NqIiIiINF8KswESHxEMQHZecYBrIiIiItJ8KcwGSHyEA4As9cyKiIiI1JvCbIDEO8vDrHpmRUREROpPYTZAfhlmoJ5ZERERkfpSmA0Qb8+shhmIiIiI1JvCbIBU9Mwe0DADERERkXpTmA2QigfANMxAREREpP4UZgOkYpjBgbwSPB4zwLURERERaZ4UZgOkVbgDw4Ayj8nhQq0CJiIiIlIfCrMBYrdaiAnVKmAiIiIiDaEwG0BxEZprVkRERKQhFGYDKMGpuWZFREREGkJhNoAqZjQ4oDArIiIiUi8KswH0y8IJGmYgIiIiUh8KswGkJW1FREREGkZhNoC0cIKIiIhIwyjMBlDFMIMsDTMQERERqReF2QA6fpiBaWoVMBEREZGTpTAbQBXzzJaWecgtKgtwbURERESan4CG2WXLljF+/HiSk5MxDIMFCxac8Jz09HT69++Pw+GgU6dOzJkzp9Hr2ViC7VacwTZACyeIiIiI1EdAw2xBQQF9+/blueeeq1P5HTt2MG7cOM477zzWr1/P7bffznXXXcdnn33WyDVtPFo4QURERKT+bIF887FjxzJ27Ng6l3/xxRdp3749TzzxBADdu3dnxYoVPPXUU4wePbqxqtmo4p0Otmbnq2dWREREpB4CGmZP1qpVqxgxYkSlfaNHj+b222+v8ZySkhJKSn7p9czNzQXA5XLhcrl8Wr+K653MdVuFBQGQkVPo8/pI01Kf9iGnBrUNqY3ah9SkJbeNk7mnZhVmMzMzSUhIqLQvISGB3NxcioqKCAkJqXLOo48+ysyZM6vsX7RoEaGhoY1Sz7S0tDqXzTtgASys/m4zrXM3NUp9pGk5mfYhpxa1DamN2ofUpCW2jcLCwjqXbVZhtj7uvfde7rjjDu/r3NxcUlNTGTVqFE6n06fv5XK5SEtLY+TIkdjt9jqdk7VyF0v2byY8NpkLLujj0/pI01Kf9iGnBrUNqY3ah9SkJbeNir+k10WzCrOJiYlkZWVV2peVlYXT6ay2VxbA4XDgcDiq7Lfb7Y32gz+ZaydGlfcOHygobXENUarXmG1Pmje1DamN2ofUpCW2jZO5n2Y1z+ygQYNYvHhxpX1paWkMGjQoQDVquIolbQ9oNgMRERGRkxbQMJufn8/69etZv349UD711vr169m9ezdQPkTgmmuu8Za/6aab2L59O/fccw8//fQTzz//PO+88w6///3vA1F9n6gIs9la0lZERETkpAU0zK5Zs4Z+/frRr18/AO644w769evHAw88AEBGRoY32AK0b9+ejz/+mLS0NPr27csTTzzBK6+80myn5QKIPzbPbEGpm4ISrQImIiIicjICOmZ22LBhmKZZ4/HqVvcaNmwY69ata8Ra+Ve4w0ZYkJWCUjfZeSW0dzSrYcwiIiIiAdWsxsy2VBW9s1kaaiAiIiJyUhRmm4C4inGzeghMRERE5KQozDYBeghMREREpH4UZpuA+IjyYQaanktERETk5CjMNgHxTg0zEBEREakPhdkmwDvMIE/DDEREREROhsJsE1AxzCA7Vz2zIiIiIidDYbYJSNAwAxEREZF6UZhtAip6Zo8WuSh2uQNcGxEREZHmQ2G2CXCG2Aiylf8oNKOBiIiISN0pzDYBhmHoITARERGRelCYbSJ+WThBPbMiIiIidaUw20R4ZzTQMAMRERGROlOYbSJ+WThBwwxERERE6kphtonQMAMRERGRk6cw20TEO8uHGWRpmIGIiIhInSnMNhG/9MxqmIGIiIhIXSnMNhEVD4BpnlkRERGRulOYbSIqHgA7VFCKy+0JcG1EREREmgeF2SYiJjQIm8UA4GC+emdFRERE6kJhtomwWAxahWtGAxEREZGToTDbhFQMNcg4WhTgmoiIiIg0DwqzTUiv1pEALN6UHeCaiIiIiDQPCrNNyITTWgPw6Q+ZFJW6A1wbERERkaZPYbYJOaNtNCnRIeSXlPH5pqxAV0dERESkyVOYbUIsFsPbOzt/3b4A10ZERESk6VOYbWIm9CsPs0u3HOCQpugSERERqZXCbBPTKT6cPimRuD0mH32XEejqiIiIiDRpCrNNUMVQg/c01EBERESkVgqzTdD4vslYLQYb9uSw/UB+oKsjIiIi0mQpzDZBcREOzuncCoAF6p0VERERqZHCbBN1ybEHweav34dpmgGujYiIiEjTpDDbRI3qkUhYkJU9h4tYu/tIoKsjIiIi0iQpzDZRIUFWRvdKBOC9tRpqICIiIlIdhdkmbGK/FAA++i6D0jJPgGsjIiIi0vQozDZhgzrGEh/h4GiRiy82Zwe6OiIiIiJNjsJsE2a1GFx8WjIAc1fvDnBtRERERJoehdkm7qoz22CzGCzdcoDPNmYGujoiIiIiTYrCbBPXMS6cG4Z2AODB9zeSV+wKcI1EREREmg6F2Wbgd8M70zY2lMzcYp5YtCXQ1RERERFpMhRmm4Fgu5WHJ/QG4LVVO1mneWdFREREAIXZZuPszq2Y2K81pgn3vvc9Lrem6hIRERFRmG1G7hvXnehQOz9l5vHPFTsCXR0RERGRgFOYbUZiwx3cN64HAH/7fAu7DxUGuEYiIiIigaUw28xc2r81gzvGUuzycN+C7/F4zEBXSURERCRgFGabGcMwePiS3gTZLCzfepB73/setwKtiIiInKIUZpuh9q3CmH1pHywGvL1mD79/e70eCBMREZFTksJsMzWhX2uemdQfm8Xggw37mfbmWkrK3IGuloiIiIhfKcw2Y+P6JPHS/5xOkM3Coh+zuOH1bykqVaAVERGRU4fCbDM3vHsC/5oygBC7laVbDjD11a/JLykLdLVERERE/EJhtgU4u3MrXv/fMwl32Fi94zATn/+SHQcLAl0tERERkUanMNtCDGgXw9zrBxIf4WBLVj4XPbOCz3/MCnS1RERERBqVwmwL0iclio9uPZsz2kaTV1LGda+v4clFmzV1l4iIiLRYCrMtTLwzmLnXn8XUwe0A+PuSbfzva9+QU1ga2IqJiIiINAKF2RYoyGZhxkU9eerKvgTbLaRvPsBFz37J5sy8QFdNRERExKeaRJh97rnnaNeuHcHBwQwcOJCvv/66xrJz5szBMIxKW3BwsB9r23xc0i+F/948mJToEHYfLmTi81/y2cbMQFdLRERExGcCHmbffvtt7rjjDh588EHWrl1L3759GT16NNnZ2TWe43Q6ycjI8G67du3yY42bl57JkXww/WwGdYiloNTNjW98y9Ofb8WjcbQiIiLSAgQ8zD755JNcf/31XHvttfTo0YMXX3yR0NBQ/vWvf9V4jmEYJCYmereEhAQ/1rj5iQkL4vX/PZMpg9oC8NTnW5g2dy0Fmo9WREREmjlbIN+8tLSUb7/9lnvvvde7z2KxMGLECFatWlXjefn5+bRt2xaPx0P//v155JFH6NmzZ7VlS0pKKCkp8b7Ozc0FwOVy4XK5fHQneK95/Nem5k8XdKVLfBgzPtrEpz9ksuNAPn+e0JO+KZGBrtopoam3DwkctQ2pjdqH1KQlt42TuSfDNM2A/b15//79tG7dmpUrVzJo0CDv/nvuuYelS5eyevXqKuesWrWKrVu30qdPH44ePcpf//pXli1bxsaNG0lJSalSfsaMGcycObPK/rlz5xIaGurbG2omtufCP7dYyXcZAPSI8nBBqofU8ABXTERERAQoLCzk6quv5ujRozidzlrLNrsw+2sul4vu3bszadIkHnrooSrHq+uZTU1N5eDBgyf8cE6Wy+UiLS2NkSNHYrfbfXptX8vMLeapz7exYP1+KobPnt81jt+d35Geyb79XKRcc2of4l9qG1IbtQ+pSUtuG7m5ubRq1apOYTagwwxatWqF1WolK6vySlVZWVkkJibW6Rp2u51+/fqxbdu2ao87HA4cDke15zXWD74xr+0rqbF2nryyH7cO78Izi7eyYP0+lmw+wJLNBxjXJ4n/u6A7raNCAl3NFqk5tA8JDLUNqY3ah9SkJbaNk7mfgD4AFhQUxOmnn87ixYu9+zweD4sXL67UU1sbt9vN999/T1JSUmNVs0Vr3yqMJ688jbQ7zmXCackYBnz8XQYjnljKc19so6TMHegqioiIiNQo4LMZ3HHHHfzjH//gtddeY9OmTdx8880UFBRw7bXXAnDNNddUekBs1qxZLFq0iO3bt7N27Vp+85vfsGvXLq677rpA3UKL0DEunL9d1Y9PfncOZ7aLocjl5vHPNjP6qWUs+SnrxBcQERERCYCADjMAuPLKKzlw4AAPPPAAmZmZnHbaaSxcuNA73dbu3buxWH7J3EeOHOH6668nMzOT6OhoTj/9dFauXEmPHj0CdQstSvckJ2/feBYfbNjPwx9vYuehQn47Zw3ndY3j+qEdGNQhFsMwAl1NEREREaAJhFmA6dOnM3369GqPpaenV3r91FNP8dRTT/mhVqcuwzC4+LTWDO+ewDOLt2KufJaIn/P5x9bOzI7uw4Vn9eKy01OICg0KdFVFRETkFNckwqw0TeEOG/de0J3SrV8TdGRL+c4C2JGWQHpaF8qST6dt32H07DeIUC0pLCIiIgGgMCu1M02Czp4Oe1bj2fM1lkNbaW/Joj1ZkLkcMv9G4UIHPzq6UpLYn1bdhpDS+1yMCK3KJiIiIo1PYVZqZxhw+hQ4fUr504KFhzH3fUvmxuXkbVtFUv4PRBiF9Cj9DnZ/B7vnwCJwRaRibzsQUgZA6gBI6A02DUsQERER31KYlZMTGoPReSRJnUeSBJgeN7u3bmD3hqW4d68mKe8HOrEXe94e+GEP/PBu+XlWBySfVh5uUwawOag7Dy3NITY8iAfH9yQmTEFXRERETp7CrDSIYbHSpmt/2nTtD8DhglKufW0ZZXvX0N+yjSsSM0gp2IhRdAT2rC7fgK7AbDOGdZ5OvL61O8NHjKP3GeeCXWNvRUREpO4UZsWnYsKCePmG87hvfhzPfLuXZ/bCbwam8uCQELav+4Ifv1lMl9JNdDN2k2wcJtn6Nbi/hs9ew73IhpHUF0vqmZByRnkvblSb8qEOIiIiItVQmBWfc9isPH5ZHzrHh/OXhT/x79V7WLY1lN2HWwPXkBQZzCMXtOe8yP2U7vyKrWu/IP7od8SRC/u/Ld9WH7tYeIJ3aAIpAyC5HwSFBvL2REREpAlRmJVGYRgGN57bkQ5x4dz21jp2Hy7EMGDKoHbcNbor4Q4b0IGgdmfTc9hdfLRhH8/OX0KX0k0MtP/M6Ki9xOb9hJGfBT99VL4BGFZI7AUpZx4LuGdATAf13oqIiJyiFGalUY3skcB/bx7M29/s4eLTkunXJrrachf2bU3f1Eu5/e313LfrCPdlwYCUEB4b5KFD8Y+w95vyLS8DMjaUb9/8o/zk0NjKvbet+4MjwnvtPYcL+WJzNkcKXMRFOIiPcBDvdBAfEUxseBB2a8BXdRYREZF6UpiVRtc9ycmMi3qesFxqTCjv3DiIN1bt5K+LtvDN3iJGvAvXDBrOnRffTITDBrn7ykPtnmPhNmM9FB6CLQvLN8A0LBRHdWGLvRuf57Xlk5wUtptJmFQNrUE2C9cObsdtIzoTGqT/HERERJob/faWJsVqMZg6pD0X9E7ioY838eGG/cxZuZNPvs/ggt5JRIXaiQ7tR1TimUS2t2PxlFK0ez22/d8QfXgDKQUbifdkE3LkJ/ryE32BOx2Qb4SzL6wnm2zdWOvuxJfFbdlZYKe0zMNLy7bz4Yb9PHhRT0b1SMDQkAUREZFmQ2FWmqR4ZzDPTOrHFWek8MD7G9lxsIA5K3fWUNoAzjy2QTxHGOTYwYUxe+hn2Ubs0R8JL8una/5qurKaCcfOMVt3JSOsBx/uDWFjXjQv/vt7FnbqwR0ThpAaG+aHuxQREZGGUpiVJu2cznF8ets5LFi3j92HC8kpcpFTWEpOoYsjhS48HpPW0SG0jgohJTqE1tEhpESH0iPJSZDt2LACtwuyfoC9a2DP1+XDE47swDjwE8kHfuJGgIo1G/ZA4d8dZAUn4YlsS0h8e5xJHbFEt4PothDVFkKiaq2z22PicnsItlsb74MRERERQGFWmoFgu5WrzmxT/wtY7eVTeiX3gzOvL99XcPDYmNsNcGQX5OzCdWgH1vwMQo0SQkt2QvZOyF4KP/y6QpHlobYi3B77vjgshXlb4bkV+8kpdDGkUyvG9UlidM9EIkPs9a+/iIiI1EhhVk5NYa2g69jy7Rg7YJaVsHLtBrZv/ZGCrG0YObtJNLNJNQ6QYmQTZ+RC8VHI/K58O04wcC1woRnJHlsce3bEs2d7HLMXxBHdujM9u/fC7S7z622KiIi0dAqzIscxbA4Gn3kmg88sH3/r9phsy85nw94cntuYxaqfdpHMQVKNbLo6DjM4Jp+yw7uId2eSahzAaRQSZxwlzjhKf7b9cuGs8s2NgWtbEva4Dsf17Lb55fuIJLBoqjAREZG6UpgVqYXVYtA1MYKuiRFccUYqGUd78s43e3n7m90sOVrMC/vKyyVHBjP9/M5c1iOcoLzdkLPLO3whP2s7JQd2EFa0j2BKsRbuh137YdeKKu9nWoMoDk3mkD2J0ohUjOh2BMe1JyKxI2EJHTDCWp1wgQi3x2TnoQKSI0MICdK4XRERadkUZkVOQlJkCLeN6Mz08zuRvjmbJT9l0z3JyeVnpOCwHQuOETGQfJr3nPBj26GjBfz+lffIOniQFOMAXR2HGZdaSpKZhevQTkIKM7C6SwnJ20kKO+HwKthV+f0LjRDyg5Mgqi3OpE4Ex3WAqDbkhbRm+cEwPv+5kPQtBzhcUEqQzcKZ7WIY2qUVQ7vE0TUhQtOOiYhIi6MwK1IPVovB8O4JDO+eUOdznKFBjOnkJHHCGO7/4Ec+yMrn8S3HXRM3iRymV2gOg2PziSjeh7N4PzGuTJLMbBI4QihFhBZth6LtkPGF99wI4AJgkBnOVDOOfUHx7PXEkrkjhvXbY1j4aQylYUlExaeSV2qSX1JGfkkZBSVurBaDO0Z2YcrgdrXWP7fYxZbMPPq1icZqUSgWEZGmQWFWxM/6t4nio1vP4R/Lt/P3xVspdXvokxLF+V3jOa9bHL2SI7H8KiwWu9zsPnyUbVs3sefnTeRmbMORv5cUo+LhtAPEGnlEG/lEG/n0YQdVFjwrA/c+gwNEkWnGkGHGkGHGklEWw5qPYrDs6cXkkYOwOJPBFlTp1LW7jzDtzbVkHC2mfaswrj+nAxP7t9b0YyIiEnAKsyIBEGSzMO28TkwZ3A5XmYfosKBaywfbrbRLiKFdwhA4ewgAhwtK+XrHYTYXu4jpEEtsmBtydh8bq7sbcvdC7n7I3Y/n6D7Iy8DqcZHIERKNI5zGz5XfZNOxDSAsHpzJmM5kNhU4+XynwZmeaDKNWDIOxTBz/hGeTNvM1MHt+M1ZbQkJsrIpI4/1u4+wYe9RNuzJIafIRWp0CCkxobQ5trWNCeX0dtG/DMkQERFpIIVZkQAKd9jAUb9zY8KCGNMrsfLOhJ7l269YADweKDwIufvg6L5jQbf864H9Oyg8uJtEDuMwXFCQDQXZGBnr6QH0qOZfikOuCDLTY1i7tBUZnmj2eWLINGPIJBbTjKHQjGFDQSkb9h6tdF58hINrh7Tn6oFt/Dr/rmmarNp+iNyiMkb2SNBQCRGRFkJhVuRUYbFAeHz5ltyv0qE4YNmWA1zw7zUEleQwuFUxTlc21vwMki1HGJVSRsfgXIzc/eVBuKyIWCOPWCOPnuwCK+Xbr7jskeQ54jloacV+TwybCiPYXujky0WxfPpFHGf378Nvzu1FclQIRwpK2ZSRy48Zufy4P5dDBaVM7N+aC/skNzh4btiTw18+/YlV2w8B0C0xgj+M6cawrnF6KE5EpJlTmBURAIZ2iePtGwcz9dVv+PhgCRBP66gBPHt1Pzq1if6loGlC0RHI3Y+Zu4+MPT8TVpKNszS7POxW9PiW5mN3HSXGdZQYttIFGAblq1NUWAd5a0PYbmnFnrJoMswY8onBbsaCGcNzW2OYsySVG0f2Y3SvpErBs9jlJn1zNh9uyGDPkUL6pEQysH0sAzvEEB8RDMCOgwX89bPNfPx9BgBBVgsOm4WfMvO4ds43DGwfwx/HdqPf8fcnIiLNisKsiHj1ah3J/FsGc+d/NpDgDGbWRT2rjuc1DAiNgdAYjMReJHep5kKmCSW5lYYyHP+9eXQfZTl7sbvyiDCKiDD30MG6p/pK5ULBuw72zo8jNK4NruA4fsoLYt0hOxllYRSbTqymk6X7nMz/ykkBwXRoFU6HuHDSN2dT5jExDJjYL4Xfj+xMuMPGC+k/8+rKnazecZhLnl/J6J4JjOieQLdEJ53iw6udn9c0TXKLyyhxuYmLcKhHV0SkiVCYFZFKUmNCeefGQQ27iGFAcGT5Ft+96mGOddCW5LFjxzZKD++hjS2HkKKsKg+uWYqPEGaUEObZC1l7AUgEhnkvUlmxaedQnpNDuU5+Y3Vij46nW6f2xMa1hp3fQ1gc9/ZpxW97duDpr47w1vpDfLYxi882Znmr3i42jC4J4ditFrJzS8jKKyYrt5hilweA1lEhnN2pFYM7xTK4YyviIuo58FlERBpMYVZEAscRQftu/YB+1R62AJQWkpO9i49XfMv3P24k0ZbPgFZuujpLiCUXo/AgFByEggNQVkSw4aI1h2htlI+PpQDYUPXaCcAjwJ/Dgsm1RHHIdLLfFU6WO5yDOU4OH4ngkBlJKBE4zUjKzAgOEkmpEcS+nCLeXrOHt9eU9yZ3TYhgWNc4RvVMpF9qVJWp1UREpPEozIpI0xYUSlRKdyZf1R3TNDFNag6LpQXHgu3B8pkbCg78EnQLDx33+tg+dwmWsmKiyCSKTDpCtQ+yHc+0h1EcFM1hnOwtDWNXUSiHDjk59KWTf69w8lpILF07tOf0Hp3p370zQcGh1V/HNNl7pIi1u4+wZudhdu20YNmYxeBOccSGV+7pdXtM1u85whc/HWD5toMcLijB7TZxmyZuj0mZxyTRGcwTV/SlZ3LkyX/GIiLNmMKsiDQbhmFQ61DVoLDyLbrtiS9mmlCaX0P4reG1uxTDVUCIq4DWQGtg4K//FXUDW49t70MBIeRaoyiyRVMaHIPLEcve0lA25TrYVRzKYZwcNp0cMcP5v7dWkU8wXRIiOatDDB3jw/l6x2GWbz3I0SJXrbeTU+ji2le/Yf60IbSOCjnx/YuItBAKsyJyajIMcESUbzHtT1y+4qG2SmG3cq+vJ/8ghUcycecfIKzsCDbchFFEmLsI3BlQUn6p3sBYgGrWyvCYBvk5IeSuDSXPDKEboVxihlIcHEZEZAzxcfFERMWCw4nH4cRwOCkLiuCRxfv4/iDc/M903rjpfCLDNI5XRE4NCrMiInVx/ENtsR2rLWIBwo9973F7yDyYzeHsfeQeyqToSCaludkYhQdJsheQbM8nyszFWngQs+AAnsLDWM0yLIaJk0KcFJY/KXe8o8e2arwMEAzkgedxC2ZwBIYjEoKd4HBW/hoc+at9v37thKBwau8GFxFpGhRmRUQagcVqITEhkcSExBOWLXO5+OTjj7lg1PnY3UXlPcDFuVBy9NjX3OO+Hq38/XHHzOJcDI8LC55j5Y7WGH5PyLCW91oHO6HaUPzrfdWUCQpTIBaRRqcwKyLSFBgG2EMg1AkRCfW7hGny1ea93PHGMkI8BfzmtGiuPT0GsziXorwjFOYdpijvMK6CHMoKj5aH35JcbK48gj0FOI1CQj0FWEw3mG4ozinf6n1P1vIeXlsQWB3Hvh632RzHfV+x3wFW+6+OHdtndVT+3lvuBOf8+voWS/3vSUSaHIVZEZGWwjA4q1sq91w2jNvfXs/MdfDSz3C4IJhSdwLlE5KdiEkIJaSGljGotQ2K89iTmUmIu4AIo5AICnEe+xphFOKkiBhbMYmOUiKNIoI9+VhLczFMT3kgLjnqHSvcZFhsVUK1aQ3CY7FjsTswrLWFb/svxyz2Y8eOXc9iP3bcXsPrivc9tq8u5yt4i5yQwqyISAszoV9r9h8tYvbCzWTmFnv3O4NtxEU4iItwkOgMJsEZTLwzmPgIB+EOG9/tPcrXOw/x7a4jbCn0sGUrlA/EjSM5MpihXeLo1yWOTvHhrN5+iE82H2Dlz4coKnJD0S/vbxgmXaIs9G5l0C3GQptIKylOG8kRFiLtJobbhekuIScvn4xDuWTl5HIwJ4/CwiLMshLMshJwl4K7FKvHRUwwtAoxiHaYRAWZOO0mDksZlmNlKCsBtwvc5eeZZaV4ykqgrBTDXYrF/NVMEJ6y8s1V+EudOeGsbIFhWLFZ7VzgMbD9FHKSYdiG22LnSLFJaEgIocHBJyz/y3Xtv3ptK3990sfsGmoijU5hVkSkBbplWCeGdGyFxzSJi3DQKtxBsL32uHZet3igM6VlHn7Yf5Q1Ow8TZLVwduc4OsaFVVrCt0tCBP8zqB3FLjff7DzMsi0H+DEjl82Z+RzML2HzEZPNR0zAA5RR0T0b4bDROjqE/TlucovDKX9kLrn2mykFcqvuDrJaCAmyEhpkJSTIisUwOJBXUs00ZiZBlGGnjCBc5d8bZThwYcd9bJ8Lu+E+tq/sl/JGefmgY+eW76v43o0NN3bKsBtuoh3gDIIwm4cQq4dgi5tgiweLx0VpaQmu0hLcZeVh24qbIMoItnhwWNzYzDIMjwsD81dVd2OUucsXuysq4mRZgVYnfZZvmYYFo7qga7EdC8LVHbPWGKxNi5XcUtifV4ZhtZMS6yQ8NOSE52GxHgvWBhiWWjbj2Fb+2sRg39ESDhWUERnqwBkahDPUgc1S2zWqXqfajROVMSp/7+ufzYnm7m4mFGZFRFqovqlR9TovyGahf5to+reJPmHZYLuVczrHcU7nOO++Q/klbM7KY0tmHluz89l5qICdBwvZf7SIvJIyfsrMA8BqMejQKoyuiRF0S4wgNSaUcIeN0CBb+VdHeUDdfiCfnzLz2JKVx+bMPH4+kI/LbVLq9lBa5Kl2Dt5gu6W85/lYr7PDZsVht+CwWXDYrIQ5bCQ4y3uo453BJEYGExliZ1t2Pt/tzeG7vUf5bm8O27Lz8ZgQF+GgfWwY7VqF0jY2jMiwIH4+Vq9NGbkczC+F2qcCrsRigOdXuTXIaqFTq2AiHRBuN4mwmYTbTSyeUjbt3E9uiQc85SHbhpsgo+yXME0ZsSEGYTaTo/kFx+13ExFkUlZait0ofx1qddMpNpjkCCslJSUUFxdTUlJMaWkJblcJFrM8pNsMt/e97LixesO7G7txrMyx497N8FS5V8P0HOs19814EwOIPLYBsMMnl631/VKObYFmYsE8Fm4Nw4LNYjDO7cEcsQXsUZXKejwm2Xkl7D1SyN4jRew9UkhWbgkH8ko4kH/sa14JxWVuYsOCiIsIJi7CQfyx7fS20Qzp1OqE/xPcFCjMioiIT8WGOxgc7mBwx8p9gsUuN7sPF7LvSBEJzmA6xofhsJ34F2X7VmEM7/7LeN8yt4f8kjIKS90UlropKnVTWFqG21PeCx3vDMYZbKvUk1xXp6VGcdpx/xNQWFqGaUKYo/Zfl9l5xfyUkcf+nCKy80rIyi0mK7eE7LxiSlweOsWH0yUhgq6J5VtqdAg7Dhawesfh8m37IbLzSvgxq7CGd0gCyoeK9EyOpGeyExPYmp3P5qw89h8tLl+6mfL/SejfJophXeM5t0scPZOd7DlcxHvr9vLW2n3sPlwI++v2eViM8v+5MU0oKasaVIOsFjrEhdE5IYIu8eF0jg+lsLiEnzNz+Dkrhx1ZRzicV1Tei31cOD4+IB8fjIMMN1EOAzxlGJ4yLB4Xhll+rKIn3Ep5j3ebKDum28Wh3IJK13NYPFjMskrn2HBjwYMFE4thYmBiNUxCbRZsFhObBWxG+T4LJgUlLlxl7vLyx8oGWcH0eDDN8v3Gsa2ijFFxfe9W/nlZjttfqYxhVvk8T8TAc2w8+rEd7vIgd/rs5WALxm61lP+8MMk6WkKpu+rPrDoH80s5mF/KpozK+0ODrAztHMfIHgmc3y2e6LBqJsduAhRmRUTEL4LtVrokRNAlIaJB17FZLUSFBhFV/UrBPhUaVLdfk/ERwcRHBJ/UtTsnRNA5IYLfnNUW0zTZeaiQ3YcLKSqtHNSLS10c2b2Z/7lwGO3iIqoN6fklZfycnc/hwlL6p0YTGWqvdLxNbCi3j+jCbcM7s2bXEd5bu5cdBwtIjgohJTqU1OjyrynRIYQ7bATZynuxbdZfHkDzeExKyjwUudwUudx4PCZJkcGVylQnp7CUPYeLKC5zU+LyUOxyU1LmobC0jD1HithxsIAdB/PZcaCAglI31JDnO8aFMbRLHEO7xHFW+1hCgsr/R+hgfglf/JTN4k3ZLNt6gMJSN1aLQae4cLolRdA9yUnXxAgOFpTyw75cfth3lI37j1JQ4q714USHzcJ5XeMZ2zuR87vFExFc/pm6PSY5haUcLiglM7eYn7Pz2ZKdz7asfLZk55FTWHMXfViQlcgQO84QO85gGxEOK5lHi9h5MA9XmftYOK4cfu0WaBsTSvvYYErL3OQUFJNTUMLRwhI8Hg8GHg6bYJaUVnk/q8UgOSqYlKhQWkeHkBwZ7B033yq8/GuI3crB/FKy84o5kFfi7c1N33yAjKPFLNyYycKNmVgtBgPaRfPg+J50T3LW+jP3N4VZERGRADMMg/atwmjfKqzKMZfLxSef/ERKdEiNvc3hDludhpUYhsGAdjEMaBdz0nW0WAxCjo1PPhnl/+Nx4h490zTJyi0hp6gUu9WC3WLBbjOwWSwE2y3eMPlrrcIdXH5GKpefkUpJmZu9R4pIiQ6pttd/Yv/yrx6PyY5DBew6VEB+iZuCkjLyi8vILymj2OWmT0oUw7rGVdsjb7UYxIY7iA130DkhotIQG9M0OZhfSrHLjc1qlN+H1YLdahBktdQY/N0ek71HCtl6LBAXlbrpFB9O18QI2req/i8YpmlyKK+IDxemcc7QoXgMC6VlnmM96CaJkSEkRDhO+D8bAPHOYHpQOaCapsnG/bks2pjJoh+z+Ckzj6+2Hya6Dj9Lf1OYFRERkYAzDIPEyPLxy/XlsFnpGBd+wnIWi0HHuPA6lT0ZhmEQF3HyS0lbLQZtY8NoGxvGiB51m2faMAwiQ+zEOKBtbCh2e/Vhv74Mw6BX60h6tY7kjlFd2X2okG93H27Qz6exKMyKiIiISK3axIbSJtYPY3vqQbMxi4iIiEizpTArIiIiIs2WwqyIiIiINFsKsyIiIiLSbCnMioiIiEizpTArIiIiIs2WwqyIiIiINFsKsyIiIiLSbCnMioiIiEizpTArIiIiIs2WwqyIiIiINFsKsyIiIiLSbCnMioiIiEizpTArIiIiIs2WLdAV8DfTNAHIzc31+bVdLheFhYXk5uZit9t9fn1p3tQ+pCZqG1IbtQ+pSUtuGxU5rSK31eaUC7N5eXkApKamBrgmIiIiIlKbvLw8IiMjay1jmHWJvC2Ix+Nh//79REREYBiGT6+dm5tLamoqe/bswel0+vTa0vypfUhN1DakNmofUpOW3DZM0yQvL4/k5GQsltpHxZ5yPbMWi4WUlJRGfQ+n09niGpX4jtqH1ERtQ2qj9iE1aalt40Q9shX0AJiIiIiINFsKsyIiIiLSbCnM+pDD4eDBBx/E4XAEuirSBKl9SE3UNqQ2ah9SE7WNcqfcA2AiIiIi0nKoZ1ZEREREmi2FWRERERFpthRmRURERKTZUpgVERERkWZLYdaHnnvuOdq1a0dwcDADBw7k66+/DnSVpBHNmDEDwzAqbd26dfMeLy4uZtq0acTGxhIeHs6ll15KVlZWpWvs3r2bcePGERoaSnx8PHfffTdlZWX+vhXxgWXLljF+/HiSk5MxDIMFCxZUOm6aJg888ABJSUmEhIQwYsQItm7dWqnM4cOHmTx5Mk6nk6ioKP73f/+X/Pz8SmW+++47zjnnHIKDg0lNTWX27NmNfWviAydqH1OnTq3y78mYMWMqlVH7aHkeffRRBgwYQEREBPHx8UyYMIHNmzdXKuOr3yXp6en0798fh8NBp06dmDNnTmPfnt8ozPrI22+/zR133MGDDz7I2rVr6du3L6NHjyY7OzvQVZNG1LNnTzIyMrzbihUrvMd+//vf8+GHH/Kf//yHpUuXsn//fiZOnOg97na7GTduHKWlpaxcuZLXXnuNOXPm8MADDwTiVqSBCgoK6Nu3L88991y1x2fPns3f//53XnzxRVavXk1YWBijR4+muLjYW2by5Mls3LiRtLQ0PvroI5YtW8YNN9zgPZ6bm8uoUaNo27Yt3377LY8//jgzZszg5ZdfbvT7k4Y5UfsAGDNmTKV/T+bNm1fpuNpHy7N06VKmTZvGV199RVpaGi6Xi1GjRlFQUOAt44vfJTt27GDcuHGcd955rF+/nttvv53rrruOzz77zK/322hM8YkzzzzTnDZtmve12+02k5OTzUcffTSAtZLG9OCDD5p9+/at9lhOTo5pt9vN//znP959mzZtMgFz1apVpmma5ieffGJaLBYzMzPTW+aFF14wnU6nWVJS0qh1l8YFmPPnz/e+9ng8ZmJiovn444979+Xk5JgOh8OcN2+eaZqm+eOPP5qA+c0333jLfPrpp6ZhGOa+fftM0zTN559/3oyOjq7UPv7whz+YXbt2beQ7El/6dfswTdOcMmWKefHFF9d4jtrHqSE7O9sEzKVLl5qm6bvfJffcc4/Zs2fPSu915ZVXmqNHj27sW/IL9cz6QGlpKd9++y0jRozw7rNYLIwYMYJVq1YFsGbS2LZu3UpycjIdOnRg8uTJ7N69G4Bvv/0Wl8tVqU1069aNNm3aeNvEqlWr6N27NwkJCd4yo0ePJjc3l40bN/r3RqRR7dixg8zMzErtITIykoEDB1ZqD1FRUZxxxhneMiNGjMBisbB69WpvmaFDhxIUFOQtM3r0aDZv3syRI0f8dDfSWNLT04mPj6dr167cfPPNHDp0yHtM7ePUcPToUQBiYmIA3/0uWbVqVaVrVJRpKRlFYdYHDh48iNvtrtSQABISEsjMzAxQraSxDRw4kDlz5rBw4UJeeOEFduzYwTnnnENeXh6ZmZkEBQURFRVV6Zzj20RmZma1babimLQcFT/P2v6NyMzMJD4+vtJxm81GTEyM2swpYMyYMbz++ussXryYxx57jKVLlzJ27Fjcbjeg9nEq8Hg83H777QwZMoRevXoB+Ox3SU1lcnNzKSoqaozb8StboCsg0lyNHTvW+32fPn0YOHAgbdu25Z133iEkJCSANROR5uaqq67yft+7d2/69OlDx44dSU9PZ/jw4QGsmfjLtGnT+OGHHyo9eyF1o55ZH2jVqhVWq7XK04VZWVkkJiYGqFbib1FRUXTp0oVt27aRmJhIaWkpOTk5lcoc3yYSExOrbTMVx6TlqPh51vZvRGJiYpUHRsvKyjh8+LDazCmoQ4cOtGrVim3btgFqHy3d9OnT+eijj/jiiy9ISUnx7vfV75KayjidzhbR+aIw6wNBQUGcfvrpLF682LvP4/GwePFiBg0aFMCaiT/l5+fz888/k5SUxOmnn47dbq/UJjZv3szu3bu9bWLQoEF8//33lX5BpaWl4XQ66dGjh9/rL42nffv2JCYmVmoPubm5rF69ulJ7yMnJ4dtvv/WWWbJkCR6Ph4EDB3rLLFu2DJfL5S2TlpZG165diY6O9tPdiD/s3buXQ4cOkZSUBKh9tFSmaTJ9+nTmz5/PkiVLaN++faXjvvpdMmjQoErXqCjTYjJKoJ9Aayneeust0+FwmHPmzDF//PFH84YbbjCjoqIqPV0oLcudd95ppqenmzt27DC//PJLc8SIEWarVq3M7Oxs0zRN86abbjLbtGljLlmyxFyzZo05aNAgc9CgQd7zy8rKzF69epmjRo0y169fby5cuNCMi4sz77333kDdkjRAXl6euW7dOnPdunUmYD755JPmunXrzF27dpmmaZp/+ctfzKioKPP99983v/vuO/Piiy8227dvbxYVFXmvMWbMGLNfv37m6tWrzRUrVpidO3c2J02a5D2ek5NjJiQkmP/zP/9j/vDDD+Zbb71lhoaGmi+99JLf71dOTm3tIy8vz7zrrrvMVatWmTt27DA///xzs3///mbnzp3N4uJi7zXUPlqem2++2YyMjDTT09PNjIwM71ZYWOgt44vfJdu3bzdDQ0PNu+++29y0aZP53HPPmVar1Vy4cKFf77exKMz60DPPPGO2adPGDAoKMs8880zzq6++CnSVpBFdeeWVZlJSkhkUFGS2bt3avPLKK81t27Z5jxcVFZm33HKLGR0dbYaGhpqXXHKJmZGRUekaO3fuNMeOHWuGhISYrVq1Mu+8807T5XL5+1bEB7744gsTqLJNmTLFNM3y6bnuv/9+MyEhwXQ4HObw4cPNzZs3V7rGoUOHzEmTJpnh4eGm0+k0r732WjMvL69SmQ0bNphnn3226XA4zNatW5t/+ctf/HWL0gC1tY/CwkJz1KhRZlxcnGm32822bdua119/fZXOELWPlqe6NgGYr776qreMr36XfPHFF+Zpp51mBgUFmR06dKj0Hs2dYZqm6e/eYBERERERX9CYWRERERFpthRmRURERKTZUpgVERERkWZLYVZEREREmi2FWRERERFpthRmRURERKTZUpgVERERkWZLYVZEREREmi2FWRERERFpthRmRUSakAMHDnDzzTfTpk0bHA4HiYmJjB49mi+//BIAwzBYsGBBYCspItKE2AJdARER+cWll15KaWkpr732Gh06dCArK4vFixdz6NChQFdNRKRJMkzTNANdCRERgZycHKKjo0lPT+fcc8+tcrxdu3bs2rXL+7pt27bs3LkTgPfff5+ZM2fy448/kpyczJQpU7jvvvuw2cr7LAzD4Pnnn+eDDz4gPT2dpKQkZs+ezWWXXeaXexMRaSwaZiAi0kSEh4cTHh7OggULKCkpqXL8m2++AeDVV18lIyPD+3r58uVcc8013Hbbbfz444+89NJLzJkzh4cffrjS+ffffz+XXnopGzZsYPLkyVx11VVs2rSp8W9MRKQRqWdWRKQJ+e9//8v1119PUVER/fv359xzz+Wqq66iT58+QHkP6/z585kwYYL3nBEjRjB8+HDuvfde775///vf3HPPPezfv9973k033cQLL7zgLXPWWWfRv39/nn/+ef/cnIhII1DPrIhIE3LppZeyf/9+PvjgA8aMGUN6ejr9+/dnzpw5NZ6zYcMGZs2a5e3ZDQ8P5/rrrycjI4PCwkJvuUGDBlU6b9CgQeqZFZFmTw+AiYg0McHBwYwcOZKRI0dy//33c9111/Hggw8yderUasvn5+czc+ZMJk6cWO21RERaMvXMiog0cT169KCgoAAAu92O2+2udLx///5s3ryZTp06Vdksll/+mf/qq68qnffVV1/RvXv3xr8BEZFGpJ5ZEZEm4tChQ1x++eX89re/pU+fPkRERLBmzRpmz57NxRdfDJTPaLB48WKGDBmCw+EgOjqaBx54gAsvvJA2bdpw2WWXYbFY2LBhAz/88AN//vOfvdf/z3/+wxlnnMHZZ5/Nm2++yddff80///nPQN2uiIhP6AEwEZEmoqSkhBkzZrBo0SJ+/vlnXC4XqampXH755fzf//0fISEhfPjhh9xxxx3s3LmT1q1be6fm+uyzz5g1axbr1q3DbrfTrVs3rrvuOq6//nqg/AGw5557jgULFrBs2TKSkpJ47LHHuOKKKwJ4xyIiDacwKyJyCqhuFgQRkZZAY2ZFREREpNlSmBURERGRZksPgImInAI0okxEWir1zIqIiIhIs6UwKyIiIiLNlsKsiIiIiDRbCrMiIiIi0mwpzIqIiIhIs6UwKyIiIiLNlsKsiIiIiDRbCrMiIiIi0mz9P1xDZ1LR8xovAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"./qwen_qlora_finetuned/checkpoint-2116/trainer_state.json\", \"r\") as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "logs = state[\"log_history\"]\n",
    "\n",
    "train_steps, train_losses = [], []\n",
    "val_steps, val_losses = [], []\n",
    "\n",
    "for log in logs:\n",
    "    if \"loss\" in log and \"step\" in log:\n",
    "        train_steps.append(log[\"step\"])\n",
    "        train_losses.append(log[\"loss\"])\n",
    "    if \"eval_loss\" in log and \"step\" in log:\n",
    "        val_steps.append(log[\"step\"])\n",
    "        val_losses.append(log[\"eval_loss\"])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_steps, train_losses, label=\"train_loss\")\n",
    "plt.plot(val_steps, val_losses, label=\"val_loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
