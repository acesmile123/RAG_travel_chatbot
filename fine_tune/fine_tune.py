import pandas as pd
import json
import time
import os
import re
import requests

# Configuration
INPUT_FILE = 'fine_tune.csv'
OUTPUT_FILE = 'training_data_ollama.jsonl'
NUM_VARIANTS = 10
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.2:3b"

SYSTEM_PROMPT = """B·∫°n l√† tr·ª£ l√Ω du l·ªãch th√¥ng minh, am hi·ªÉu v·ªÅ du l·ªãch Vi·ªát Nam, ƒë·∫∑c bi·ªát l√† ƒê√† L·∫°t (T√¢y Nguy√™n). 
H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, ch√≠nh x√°c v√† nhi·ªát t√¨nh. S·ª≠ d·ª•ng emoji ph√π h·ª£p üå≤‚òïÔ∏è."""


def test_ollama():
    """
    Ki·ªÉm tra Ollama service c√≥ ƒëang ch·∫°y kh√¥ng.
    Returns: True n·∫øu service ƒëang ch·∫°y, False n·∫øu kh√¥ng.
    """
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"Ollama running with {len(models)} model(s)")
            return True
        return False
    except:
        print("ERROR: Ollama not running")
        print("Install: https://ollama.com/download")
        print("Then run: ollama pull llama3.2:3b")
        return False


def generate_variants_with_ollama(question):
    """
    G·ªçi Ollama API ƒë·ªÉ sinh c√°c c√¢u h·ªèi bi·∫øn th·ªÉ.
    
    Args:
        question (str): C√¢u h·ªèi g·ªëc c·∫ßn t·∫°o variants
        
    Returns:
        list: Danh s√°ch c√°c c√¢u h·ªèi variants, ho·∫∑c None n·∫øu l·ªói
    """
    prompt = f"""Vi·∫øt l·∫°i c√¢u h·ªèi sau th√†nh {NUM_VARIANTS} c√°ch kh√°c nhau nh∆∞ng gi·ªØ nguy√™n √Ω nghƒ©a.

C√¢u g·ªëc: "{question}"

Y√äU C·∫¶U:
- PH·∫¢I thay ƒë·ªïi c·∫•u tr√∫c c√¢u, d√πng t·ª´ ƒë·ªìng nghƒ©a, ƒë·∫£o ng·ªØ.
- T·∫°o ra: 1 c√¢u Gen Z, 1 c√¢u keyword ng·∫Øn, 1 c√¢u trang tr·ªçng.
- Ch·ªâ tr·∫£ v·ªÅ danh s√°ch c√¢u h·ªèi, m·ªói c√¢u 1 d√≤ng, KH√îNG ƒë√°nh s·ªë.

Tr·∫£ l·ªùi:"""

    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.9, "top_p": 0.95}
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            text = result.get('response', '')
            lines = [l.strip() for l in text.split('\n') if len(l.strip()) > 5]
            cleaned = [re.sub(r'^\d+[\.\)]\s*', '', l).strip() for l in lines]
            return [l for l in cleaned if len(l) > 5][:NUM_VARIANTS]
        
        return None
        
    except Exception as e:
        print(f"  ERROR: {str(e)[:50]}")
        return None


def main():
    """
    Main function: Load CSV, generate variants with Ollama, save to JSONL.
    Supports resume from previous run.
    """
    print("\n" + "="*70)
    print("FINE-TUNE DATA GENERATION - Ollama Local")
    print("="*70 + "\n")
    
    if not test_ollama():
        return
    
    # Resume logic: Load processed answers
    processed_answers = set()
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                processed_answers.add(data['messages'][2]['content'])
        print(f"Found {len(processed_answers)} processed samples, resuming...\n")
    
    # Load input CSV
    df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')
    df = df.dropna(subset=['Question', 'Answer'])
    total = len(df)
    
    print(f"Total questions: {total}")
    print(f"Estimated time: ~{(total * 15) / 60:.1f} minutes\n")
    
    success = 0
    failed = 0
    
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            q_clean = re.sub(r'^\d+\.\s*', '', str(row['Question'])).strip()
            ans = str(row['Answer']).strip()
            
            if ans in processed_answers:
                continue
            
            print(f"[{idx+1}/{total}] Processing: {q_clean[:60]}...")
            
            variants = generate_variants_with_ollama(q_clean)
            
            if variants and len(variants) >= 5:
                success += 1
                print(f"  Generated {len(variants)} variants")
                
                for v in variants:
                    entry = {
                        "messages": [
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": v},
                            {"role": "assistant", "content": ans}
                        ]
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            else:
                failed += 1
                print(f"  Failed - using template fallback")
                for template in [q_clean, f"Cho m√¨nh h·ªèi {q_clean}", f"B·∫°n ∆°i, {q_clean}"]:
                    entry = {
                        "messages": [
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": template},
                            {"role": "assistant", "content": ans}
                        ]
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            time.sleep(0.5)
    
    print("\n" + "="*70)
    print(f"COMPLETED: Success={success}/{total}, Failed={failed}/{total}")
    print(f"Output: {OUTPUT_FILE}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
